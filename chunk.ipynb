{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4873a24-4e7c-4f75-9ab1-c18f2652a3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting vectorization pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📂 Checking PDFs: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Skipping (cached): temp.pdf\n",
      "✅ Skipping (cached): temp_uploaded.pdf\n",
      "🔄 Rebuilding merged vectorstore from all .pkl chunks...\n",
      "📦 Loaded total 56 chunks from chunk_batches\n",
      "✅ Vectorstore saved to: vectorstore_merged\n",
      "🎉 Done! All PDFs chunked and merged into final vectorstore.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import hashlib\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_core.embeddings import Embeddings\n",
    "\n",
    "import faiss\n",
    "\n",
    "# === Setup ===\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "load_dotenv()\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"True\"\n",
    "\n",
    "# === Config ===\n",
    "PDF_FOLDER = \"Data\"\n",
    "CHUNK_SAVE_DIR = \"chunk_batches\"\n",
    "VECTORSTORE_DIR = \"vectorstore_parts\"\n",
    "MERGED_VECTORSTORE_PATH = \"vectorstore_merged\"\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 150\n",
    "EMBED_MODEL_NAME = \"intfloat/e5-base-v2\"\n",
    "\n",
    "os.makedirs(CHUNK_SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(VECTORSTORE_DIR, exist_ok=True)\n",
    "\n",
    "# === Custom Embedding Wrapper ===\n",
    "class SentenceTransformerEmbeddings(Embeddings):\n",
    "    def __init__(self, model_name=\"intfloat/e5-base-v2\"):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.model_name = model_name  # Track model name\n",
    "        \n",
    "    def embed_documents(self, texts):\n",
    "        # Add prefix required by E5 models for passages\n",
    "        if \"e5\" in self.model_name:\n",
    "            texts = [\"passage: \" + text for text in texts]\n",
    "        return self.model.encode(texts, convert_to_numpy=True).tolist()\n",
    "    \n",
    "    def embed_query(self, text):\n",
    "        # Add prefix required by E5 models for queries\n",
    "        if \"e5\" in self.model_name:\n",
    "            text = \"query: \" + text\n",
    "        return self.model.encode(text, convert_to_numpy=True).tolist()\n",
    "\n",
    "# === Helper: List PDF files ===\n",
    "def list_pdf_files(folder):\n",
    "    return [\n",
    "        os.path.join(root, file)\n",
    "        for root, _, files in os.walk(folder)\n",
    "        for file in files if file.endswith(\".pdf\")\n",
    "    ]\n",
    "\n",
    "# === Helper: Check if .pkl is up to date ===\n",
    "def is_pickle_up_to_date(pdf_path, pkl_path):\n",
    "    return os.path.exists(pkl_path) and os.path.getmtime(pdf_path) <= os.path.getmtime(pkl_path)\n",
    "\n",
    "# === Chunk a single PDF and save ===\n",
    "def chunk_pdf(pdf_path):\n",
    "    print(f\"📄 Processing: {os.path.basename(pdf_path)}\")\n",
    "    try:\n",
    "        loader = PyMuPDFLoader(pdf_path)\n",
    "        pages = loader.load()\n",
    "        for page in pages:\n",
    "            page.metadata[\"source\"] = os.path.basename(pdf_path)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading {pdf_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        chunk_overlap=CHUNK_OVERLAP,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "        add_start_index=True,\n",
    "        keep_separator=True\n",
    "    )\n",
    "\n",
    "    chunks = splitter.split_documents(pages)\n",
    "    for chunk in chunks:\n",
    "        content_hash = hashlib.md5(chunk.page_content.encode()).hexdigest()[:8]\n",
    "        chunk.metadata[\"chunk_id\"] = f\"{chunk.metadata['source']}_{chunk.metadata.get('page', 0)}_{content_hash}\"\n",
    "\n",
    "    # Save as .pkl\n",
    "    pkl_name = os.path.join(CHUNK_SAVE_DIR, f\"{os.path.basename(pdf_path)}.pkl\")\n",
    "    with open(pkl_name, \"wb\") as f:\n",
    "        pickle.dump(chunks, f)\n",
    "    print(f\"💾 Saved {len(chunks)} chunks to {pkl_name}\")\n",
    "    return chunks\n",
    "\n",
    "# === Load all chunks ===\n",
    "def load_all_chunks():\n",
    "    all_chunks = []\n",
    "    for file in os.listdir(CHUNK_SAVE_DIR):\n",
    "        if file.endswith(\".pkl\"):\n",
    "            with open(os.path.join(CHUNK_SAVE_DIR, file), \"rb\") as f:\n",
    "                chunks = pickle.load(f)\n",
    "                all_chunks.extend(chunks)\n",
    "    print(f\"📦 Loaded total {len(all_chunks)} chunks from {CHUNK_SAVE_DIR}\")\n",
    "    return all_chunks\n",
    "\n",
    "# === Create FAISS vectorstore ===\n",
    "def create_vectorstore(chunks, embeddings):\n",
    "    sample_vec = embeddings.embed_query(\"test\")\n",
    "    index = faiss.IndexFlatL2(len(sample_vec))\n",
    "    vs = FAISS(\n",
    "        embedding_function=embeddings,\n",
    "        index=index,\n",
    "        docstore=InMemoryDocstore(),\n",
    "        index_to_docstore_id={}\n",
    "    )\n",
    "    vs.add_documents(chunks)\n",
    "    return vs\n",
    "\n",
    "# === Save vectorstore ===\n",
    "def save_vectorstore(vs, path):\n",
    "    vs.save_local(path)\n",
    "    print(f\"✅ Vectorstore saved to: {path}\")\n",
    "\n",
    "# === MAIN ===\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🚀 Starting vectorization pipeline...\")\n",
    "\n",
    "    embeddings = SentenceTransformerEmbeddings(model_name=EMBED_MODEL_NAME)\n",
    "    pdf_files = list_pdf_files(PDF_FOLDER)\n",
    "\n",
    "    for pdf_path in tqdm(pdf_files, desc=\"📂 Checking PDFs\"):\n",
    "        pkl_path = os.path.join(CHUNK_SAVE_DIR, f\"{os.path.basename(pdf_path)}.pkl\")\n",
    "        if is_pickle_up_to_date(pdf_path, pkl_path):\n",
    "            print(f\"✅ Skipping (cached): {os.path.basename(pdf_path)}\")\n",
    "        else:\n",
    "            chunk_pdf(pdf_path)\n",
    "\n",
    "    # Reload and merge everything\n",
    "    print(\"🔄 Rebuilding merged vectorstore from all .pkl chunks...\")\n",
    "    all_chunks = load_all_chunks()\n",
    "    vectorstore = create_vectorstore(all_chunks, embeddings)\n",
    "    save_vectorstore(vectorstore, MERGED_VECTORSTORE_PATH)\n",
    "\n",
    "    print(\"🎉 Done! All PDFs chunked and merged into final vectorstore.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49906919-61a4-4263-b398-07a47f899a65",
   "metadata": {},
   "source": [
    "# NEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7211ecf0-2c31-43f9-95c4-ef1d2de6302f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting optimized PDF vectorization pipeline...\n",
      "Using embedding model: intfloat/e5-base-v2\n",
      "Token chunk size: 256, Overlap: 38\n",
      "Found 0 PDF files in Publications_PDFs/AFO-50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📂 Processing PDFs: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔄 Rebuilding merged vectorstore from all chunks...\n",
      "📦 Loaded total 56 chunks from chunk_batches2\n",
      "📊 Chunk stats: Avg 889.3 chars | Min 270 | Max 999\n",
      "⚙️ Creating vectorstore (this may take a while)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Vectorstore saved to: vectorstore_merged2\n",
      "\n",
      "🎉 Pipeline complete! All PDFs processed and indexed.\n",
      "Total chunks: 56\n",
      "Vectorstore saved to: vectorstore_merged2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import hashlib\n",
    "import warnings\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from transformers import AutoTokenizer\n",
    "import faiss\n",
    "\n",
    "# === Setup ===\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "load_dotenv()\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"True\"\n",
    "\n",
    "# === Config ===\n",
    "PDF_FOLDER = \"Publications_PDFs/AFO-50\"\n",
    "CHUNK_SAVE_DIR = \"chunk_batches2\"\n",
    "VECTORSTORE_DIR = \"vectorstore_parts2\"\n",
    "MERGED_VECTORSTORE_PATH = \"vectorstore_merged2\"\n",
    "TOKEN_CHUNK_SIZE = 256  # Optimal for E5 models\n",
    "TOKEN_OVERLAP = 38      # 15% overlap\n",
    "EMBED_MODEL_NAME = \"intfloat/e5-base-v2\"\n",
    "TOKENIZER_NAME = \"intfloat/e5-base-v2\"  # Same as model for consistency\n",
    "\n",
    "os.makedirs(CHUNK_SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(VECTORSTORE_DIR, exist_ok=True)\n",
    "\n",
    "# === Custom Embedding Wrapper ===\n",
    "class SentenceTransformerEmbeddings(Embeddings):\n",
    "    def __init__(self, model_name=\"intfloat/e5-base-v2\"):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.model_name = model_name\n",
    "        \n",
    "    def embed_documents(self, texts):\n",
    "        # Add E5-specific prefix for passages\n",
    "        if \"e5\" in self.model_name.lower():\n",
    "            texts = [\"passage: \" + text for text in texts]\n",
    "        return self.model.encode(texts, \n",
    "                                 convert_to_numpy=True, \n",
    "                                 batch_size=32, \n",
    "                                 show_progress_bar=False).tolist()\n",
    "    \n",
    "    def embed_query(self, text):\n",
    "        # Add E5-specific prefix for queries\n",
    "        if \"e5\" in self.model_name.lower():\n",
    "            text = \"query: \" + text\n",
    "        return self.model.encode(text, convert_to_numpy=True).tolist()\n",
    "\n",
    "# === Helper: List PDF files ===\n",
    "def list_pdf_files(folder):\n",
    "    return [\n",
    "        os.path.join(root, file)\n",
    "        for root, _, files in os.walk(folder)\n",
    "        for file in files if file.endswith(\".pdf\")\n",
    "    ]\n",
    "\n",
    "# === Helper: Check if .pkl is up to date ===\n",
    "def is_pickle_up_to_date(pdf_path, pkl_path):\n",
    "    return os.path.exists(pkl_path) and os.path.getmtime(pdf_path) <= os.path.getmtime(pkl_path)\n",
    "\n",
    "# === Clean PDF content ===\n",
    "def clean_page_content(page_content):\n",
    "    \"\"\"Remove common PDF artifacts and noise\"\"\"\n",
    "    # Remove headers/footers with page numbers\n",
    "    page_content = re.sub(r'Page \\d+ of \\d+', '', page_content, flags=re.IGNORECASE)\n",
    "    # Remove isolated numbers (likely page numbers)\n",
    "    page_content = re.sub(r'^\\d+$', '', page_content, flags=re.MULTILINE)\n",
    "    # Remove repeated headers\n",
    "    page_content = re.sub(r'^.*\\n\\1$', '', page_content, flags=re.MULTILINE)\n",
    "    # Remove excessive whitespace\n",
    "    page_content = re.sub(r'\\s+', ' ', page_content).strip()\n",
    "    return page_content\n",
    "\n",
    "# === Validate chunk quality ===\n",
    "def is_valid_chunk(chunk):\n",
    "    \"\"\"Filter out low-quality chunks\"\"\"\n",
    "    content = chunk.page_content.strip()\n",
    "    # Remove chunks that are too short\n",
    "    if len(content) < 50:\n",
    "        return False\n",
    "    # Remove chunks that are mostly numbers\n",
    "    if sum(c.isdigit() for c in content) / len(content) > 0.3:\n",
    "        return False\n",
    "    # Remove chunks that are all uppercase (likely headers)\n",
    "    if content.isupper() and len(content) < 100:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# === Chunk a single PDF and save ===\n",
    "def chunk_pdf(pdf_path):\n",
    "    print(f\"📄 Processing: {os.path.basename(pdf_path)}\")\n",
    "    try:\n",
    "        loader = PyMuPDFLoader(pdf_path)\n",
    "        pages = loader.load()\n",
    "        \n",
    "        # Clean each page and update metadata\n",
    "        for page in pages:\n",
    "            page.metadata[\"source\"] = os.path.basename(pdf_path)\n",
    "            page.page_content = clean_page_content(page.page_content)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading {pdf_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "    # Initialize token-based splitter\n",
    "    tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)\n",
    "    splitter = TokenTextSplitter(\n",
    "        encoding_name=TOKENIZER_NAME,\n",
    "        chunk_size=TOKEN_CHUNK_SIZE,\n",
    "        chunk_overlap=TOKEN_OVERLAP,\n",
    "        add_start_index=True,\n",
    "        disallowed_special=(tokenizer.all_special_tokens)\n",
    "    )\n",
    "\n",
    "    chunks = splitter.split_documents(pages)\n",
    "    \n",
    "    # Add chunk IDs and filter low-quality chunks\n",
    "    valid_chunks = []\n",
    "    for chunk in chunks:\n",
    "        content = chunk.page_content.strip()\n",
    "        if not content:\n",
    "            continue\n",
    "            \n",
    "        content_hash = hashlib.md5(content.encode()).hexdigest()[:8]\n",
    "        chunk.metadata[\"chunk_id\"] = f\"{chunk.metadata['source']}_{chunk.metadata.get('page', 0)}_{content_hash}\"\n",
    "        \n",
    "        if is_valid_chunk(chunk):\n",
    "            valid_chunks.append(chunk)\n",
    "\n",
    "    # Save as .pkl\n",
    "    pkl_name = os.path.join(CHUNK_SAVE_DIR, f\"{os.path.basename(pdf_path)}.pkl\")\n",
    "    with open(pkl_name, \"wb\") as f:\n",
    "        pickle.dump(valid_chunks, f)\n",
    "        \n",
    "    print(f\"💾 Saved {len(valid_chunks)}/{len(chunks)} chunks to {pkl_name}\")\n",
    "    return valid_chunks\n",
    "\n",
    "# === Load all chunks ===\n",
    "def load_all_chunks():\n",
    "    all_chunks = []\n",
    "    for file in os.listdir(CHUNK_SAVE_DIR):\n",
    "        if file.endswith(\".pkl\"):\n",
    "            with open(os.path.join(CHUNK_SAVE_DIR, file), \"rb\") as f:\n",
    "                chunks = pickle.load(f)\n",
    "                all_chunks.extend(chunks)\n",
    "                \n",
    "    print(f\"📦 Loaded total {len(all_chunks)} chunks from {CHUNK_SAVE_DIR}\")\n",
    "    return all_chunks\n",
    "\n",
    "# === Create FAISS vectorstore ===\n",
    "def create_vectorstore(chunks, embeddings):\n",
    "    # Create optimized FAISS index\n",
    "    print(\"⚙️ Creating vectorstore (this may take a while)...\")\n",
    "    vs = FAISS.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embeddings,\n",
    "        # batch_size=128,  # Process in batches for efficiency\n",
    "        normalize_L2=True  # Important for cosine similarity\n",
    "    )\n",
    "    return vs\n",
    "\n",
    "# === Save vectorstore ===\n",
    "def save_vectorstore(vs, path):\n",
    "    vs.save_local(path)\n",
    "    print(f\"✅ Vectorstore saved to: {path}\")\n",
    "\n",
    "# === MAIN ===\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🚀 Starting optimized PDF vectorization pipeline...\")\n",
    "    print(f\"Using embedding model: {EMBED_MODEL_NAME}\")\n",
    "    print(f\"Token chunk size: {TOKEN_CHUNK_SIZE}, Overlap: {TOKEN_OVERLAP}\")\n",
    "\n",
    "    # Initialize embeddings\n",
    "    embeddings = SentenceTransformerEmbeddings(model_name=EMBED_MODEL_NAME)\n",
    "    \n",
    "    # Get PDF files\n",
    "    pdf_files = list_pdf_files(PDF_FOLDER)\n",
    "    print(f\"Found {len(pdf_files)} PDF files in {PDF_FOLDER}\")\n",
    "\n",
    "    # Process PDFs\n",
    "    for pdf_path in tqdm(pdf_files, desc=\"📂 Processing PDFs\"):\n",
    "        pkl_path = os.path.join(CHUNK_SAVE_DIR, f\"{os.path.basename(pdf_path)}.pkl\")\n",
    "        if is_pickle_up_to_date(pdf_path, pkl_path):\n",
    "            print(f\"✅ Skipping (cached): {os.path.basename(pdf_path)}\")\n",
    "        else:\n",
    "            chunk_pdf(pdf_path)\n",
    "\n",
    "    # Reload and merge everything\n",
    "    print(\"\\n🔄 Rebuilding merged vectorstore from all chunks...\")\n",
    "    all_chunks = load_all_chunks()\n",
    "    \n",
    "    # Analyze chunk distribution\n",
    "    chunk_lengths = [len(c.page_content) for c in all_chunks]\n",
    "    if chunk_lengths:\n",
    "        avg_len = sum(chunk_lengths) / len(chunk_lengths)\n",
    "        max_len = max(chunk_lengths)\n",
    "        min_len = min(chunk_lengths)\n",
    "        print(f\"📊 Chunk stats: Avg {avg_len:.1f} chars | Min {min_len} | Max {max_len}\")\n",
    "    else:\n",
    "        print(\"⚠️ No chunks found! Check PDF processing\")\n",
    "        exit(1)\n",
    "\n",
    "    # Create and save vectorstore\n",
    "    vectorstore = create_vectorstore(all_chunks, embeddings)\n",
    "    save_vectorstore(vectorstore, MERGED_VECTORSTORE_PATH)\n",
    "\n",
    "    print(\"\\n🎉 Pipeline complete! All PDFs processed and indexed.\")\n",
    "    print(f\"Total chunks: {len(all_chunks)}\")\n",
    "    print(f\"Vectorstore saved to: {MERGED_VECTORSTORE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e9848a-1200-4277-a446-f8b5c098b8ae",
   "metadata": {},
   "source": [
    "## Updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31075504-cccc-498f-93c0-13f7e17d3c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting optimized PDF vectorization pipeline...\n",
      "Using embedding model: intfloat/e5-base-v2\n",
      "Token chunk size: 256, Overlap: 38\n",
      "✅ Loaded embedding model: intfloat/e5-base-v2\n",
      "🔍 Found 2 PDF files in Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs: 100%|█████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 2003.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Skipping (cached): temp.pdf\n",
      "✅ Skipping (cached): temp_uploaded.pdf\n",
      "\n",
      "🔄 Loading all chunks...\n",
      "📦 Loaded 56 chunks from 2 files\n",
      "📊 Chunk stats: Avg 889.3 chars | Min 270 | Max 999\n",
      "⚙️ Creating vectorstore (this may take a while)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing chunks: 100%|███████████████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Vectorstore saved to: vectorstore_merged2\n",
      "\n",
      "🎉 Pipeline complete!\n",
      "Total documents processed: 2\n",
      "Total chunks indexed: 56\n",
      "Vectorstore saved to: vectorstore_merged2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import hashlib\n",
    "import warnings\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from transformers import AutoTokenizer\n",
    "import faiss\n",
    "\n",
    "# === Setup ===\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "load_dotenv()\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"True\"\n",
    "\n",
    "# === Config ===\n",
    "PDF_FOLDER = \"Data\"\n",
    "CHUNK_SAVE_DIR = \"chunk_batches2\"\n",
    "MERGED_VECTORSTORE_PATH = \"vectorstore_merged2\"\n",
    "TOKEN_CHUNK_SIZE = 256\n",
    "TOKEN_OVERLAP = 38\n",
    "EMBED_MODEL_NAME = \"intfloat/e5-base-v2\"\n",
    "TOKENIZER_NAME = \"intfloat/e5-base-v2\"\n",
    "EMBED_BATCH_SIZE = 32  # Optimized batch size for embedding generation\n",
    "\n",
    "os.makedirs(CHUNK_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# === Custom Embedding Wrapper ===\n",
    "class SentenceTransformerEmbeddings(Embeddings):\n",
    "    def __init__(self, model_name=\"intfloat/e5-base-v2\"):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.model_name = model_name\n",
    "        print(f\"✅ Loaded embedding model: {model_name}\")\n",
    "        \n",
    "    def embed_documents(self, texts):\n",
    "        # Add E5-specific prefix for passages\n",
    "        if \"e5\" in self.model_name.lower():\n",
    "            texts = [\"passage: \" + text for text in texts]\n",
    "            \n",
    "        # Batch processing for efficiency\n",
    "        embeddings = []\n",
    "        for i in range(0, len(texts), EMBED_BATCH_SIZE):\n",
    "            batch = texts[i:i+EMBED_BATCH_SIZE]\n",
    "            embeddings.extend(self.model.encode(batch, \n",
    "                                              convert_to_numpy=True, \n",
    "                                              show_progress_bar=False).tolist())\n",
    "        return embeddings\n",
    "    \n",
    "    def embed_query(self, text):\n",
    "        # Add E5-specific prefix for queries\n",
    "        if \"e5\" in self.model_name.lower():\n",
    "            text = \"query: \" + text\n",
    "        return self.model.encode(text, convert_to_numpy=True).tolist()\n",
    "\n",
    "# === Helper: List PDF files ===\n",
    "def list_pdf_files(folder):\n",
    "    pdfs = [\n",
    "        os.path.join(root, file)\n",
    "        for root, _, files in os.walk(folder)\n",
    "        for file in files if file.endswith(\".pdf\")\n",
    "    ]\n",
    "    print(f\"🔍 Found {len(pdfs)} PDF files in {folder}\")\n",
    "    return pdfs\n",
    "\n",
    "# === Helper: Check if .pkl is up to date ===\n",
    "def is_pickle_up_to_date(pdf_path, pkl_path):\n",
    "    if not os.path.exists(pkl_path):\n",
    "        return False\n",
    "    return os.path.getmtime(pdf_path) <= os.path.getmtime(pkl_path)\n",
    "\n",
    "# === Clean PDF content ===\n",
    "def clean_page_content(page_content):\n",
    "    \"\"\"Remove common PDF artifacts and noise\"\"\"\n",
    "    # Remove headers/footers with page numbers\n",
    "    page_content = re.sub(r'Page \\d+ of \\d+', '', page_content, flags=re.IGNORECASE)\n",
    "    # Remove isolated numbers (likely page numbers)\n",
    "    page_content = re.sub(r'^\\d+$', '', page_content, flags=re.MULTILINE)\n",
    "    # Remove excessive whitespace\n",
    "    page_content = re.sub(r'\\s+', ' ', page_content).strip()\n",
    "    return page_content\n",
    "\n",
    "# === Validate chunk quality ===\n",
    "def is_valid_chunk(chunk):\n",
    "    \"\"\"Filter out low-quality chunks\"\"\"\n",
    "    content = chunk.page_content.strip()\n",
    "    # Remove chunks that are too short\n",
    "    if len(content) < 50:\n",
    "        return False\n",
    "    # Remove chunks that are mostly numbers\n",
    "    if sum(c.isdigit() for c in content) / len(content) > 0.3:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# === Chunk a single PDF and save ===\n",
    "def chunk_pdf(pdf_path):\n",
    "    filename = os.path.basename(pdf_path)\n",
    "    print(f\"📄 Processing: {filename}\")\n",
    "    try:\n",
    "        loader = PyMuPDFLoader(pdf_path)\n",
    "        pages = loader.load()\n",
    "        \n",
    "        # Clean each page and update metadata\n",
    "        for page in pages:\n",
    "            page.metadata[\"source\"] = filename\n",
    "            page.page_content = clean_page_content(page.page_content)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading {filename}: {e}\")\n",
    "        return []\n",
    "\n",
    "    # Initialize token-based splitter\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME, local_files_only=True)\n",
    "        splitter = TokenTextSplitter(\n",
    "            encoding_name=TOKENIZER_NAME,\n",
    "            chunk_size=TOKEN_CHUNK_SIZE,\n",
    "            chunk_overlap=TOKEN_OVERLAP,\n",
    "            add_start_index=True,\n",
    "            disallowed_special=(tokenizer.all_special_tokens)\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Tokenizer error, using fallback: {e}\")\n",
    "        splitter = TokenTextSplitter(\n",
    "            chunk_size=TOKEN_CHUNK_SIZE,\n",
    "            chunk_overlap=TOKEN_OVERLAP,\n",
    "            add_start_index=True\n",
    "        )\n",
    "\n",
    "    chunks = splitter.split_documents(pages)\n",
    "    \n",
    "    # Add chunk IDs and filter low-quality chunks\n",
    "    valid_chunks = []\n",
    "    for chunk in chunks:\n",
    "        content = chunk.page_content.strip()\n",
    "        if not content:\n",
    "            continue\n",
    "            \n",
    "        content_hash = hashlib.md5(content.encode()).hexdigest()[:8]\n",
    "        chunk.metadata[\"chunk_id\"] = f\"{chunk.metadata['source']}_{chunk.metadata.get('page', 0)}_{content_hash}\"\n",
    "        \n",
    "        if is_valid_chunk(chunk):\n",
    "            valid_chunks.append(chunk)\n",
    "\n",
    "    # Save as .pkl\n",
    "    pkl_name = os.path.join(CHUNK_SAVE_DIR, f\"{filename}.pkl\")\n",
    "    with open(pkl_name, \"wb\") as f:\n",
    "        pickle.dump(valid_chunks, f)\n",
    "        \n",
    "    print(f\"💾 Saved {len(valid_chunks)}/{len(chunks)} chunks to {pkl_name}\")\n",
    "    return valid_chunks\n",
    "\n",
    "# === Load all chunks ===\n",
    "def load_all_chunks():\n",
    "    all_chunks = []\n",
    "    pkl_files = [f for f in os.listdir(CHUNK_SAVE_DIR) if f.endswith(\".pkl\")]\n",
    "    \n",
    "    if not pkl_files:\n",
    "        print(\"⚠️ No chunk files found in {CHUNK_SAVE_DIR}\")\n",
    "        return all_chunks\n",
    "        \n",
    "    for file in pkl_files:\n",
    "        with open(os.path.join(CHUNK_SAVE_DIR, file), \"rb\") as f:\n",
    "            chunks = pickle.load(f)\n",
    "            all_chunks.extend(chunks)\n",
    "                \n",
    "    print(f\"📦 Loaded {len(all_chunks)} chunks from {len(pkl_files)} files\")\n",
    "    return all_chunks\n",
    "\n",
    "# === Create FAISS vectorstore with batching ===\n",
    "def create_vectorstore(chunks, embeddings):\n",
    "    print(\"⚙️ Creating vectorstore (this may take a while)...\")\n",
    "    \n",
    "    # Create empty FAISS index\n",
    "    sample_embedding = embeddings.embed_query(\"test\")\n",
    "    dimension = len(sample_embedding)\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    docstore = InMemoryDocstore()\n",
    "    index_to_docstore_id = {}\n",
    "    \n",
    "    vectorstore = FAISS(\n",
    "        embedding_function=embeddings,\n",
    "        index=index,\n",
    "        docstore=docstore,\n",
    "        index_to_docstore_id=index_to_docstore_id,\n",
    "        normalize_L2=True\n",
    "    )\n",
    "    \n",
    "    # Add documents in batches\n",
    "    batch_size = 128\n",
    "    for i in tqdm(range(0, len(chunks), batch_size), desc=\"Indexing chunks\"):\n",
    "        batch = chunks[i:i+batch_size]\n",
    "        vectorstore.add_documents(batch)\n",
    "        \n",
    "    return vectorstore\n",
    "\n",
    "# === Save vectorstore ===\n",
    "def save_vectorstore(vs, path):\n",
    "    vs.save_local(path)\n",
    "    print(f\"✅ Vectorstore saved to: {path}\")\n",
    "\n",
    "# === MAIN ===\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🚀 Starting optimized PDF vectorization pipeline...\")\n",
    "    print(f\"Using embedding model: {EMBED_MODEL_NAME}\")\n",
    "    print(f\"Token chunk size: {TOKEN_CHUNK_SIZE}, Overlap: {TOKEN_OVERLAP}\")\n",
    "\n",
    "    # Initialize embeddings\n",
    "    embeddings = SentenceTransformerEmbeddings(model_name=EMBED_MODEL_NAME)\n",
    "    \n",
    "    # Get PDF files\n",
    "    pdf_files = list_pdf_files(PDF_FOLDER)\n",
    "\n",
    "    # Process PDFs\n",
    "    for pdf_path in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "        pkl_path = os.path.join(CHUNK_SAVE_DIR, f\"{os.path.basename(pdf_path)}.pkl\")\n",
    "        if is_pickle_up_to_date(pdf_path, pkl_path):\n",
    "            print(f\"✅ Skipping (cached): {os.path.basename(pdf_path)}\")\n",
    "        else:\n",
    "            chunk_pdf(pdf_path)\n",
    "\n",
    "    # Reload and merge everything\n",
    "    print(\"\\n🔄 Loading all chunks...\")\n",
    "    all_chunks = load_all_chunks()\n",
    "    \n",
    "    if not all_chunks:\n",
    "        print(\"❌ No chunks found! Exiting.\")\n",
    "        exit(1)\n",
    "        \n",
    "    # Analyze chunk distribution\n",
    "    chunk_lengths = [len(c.page_content) for c in all_chunks]\n",
    "    avg_len = sum(chunk_lengths) / len(chunk_lengths)\n",
    "    max_len = max(chunk_lengths)\n",
    "    min_len = min(chunk_lengths)\n",
    "    print(f\"📊 Chunk stats: Avg {avg_len:.1f} chars | Min {min_len} | Max {max_len}\")\n",
    "\n",
    "    # Create and save vectorstore\n",
    "    vectorstore = create_vectorstore(all_chunks, embeddings)\n",
    "    save_vectorstore(vectorstore, MERGED_VECTORSTORE_PATH)\n",
    "\n",
    "    print(\"\\n🎉 Pipeline complete!\")\n",
    "    print(f\"Total documents processed: {len(pdf_files)}\")\n",
    "    print(f\"Total chunks indexed: {len(all_chunks)}\")\n",
    "    print(f\"Vectorstore saved to: {MERGED_VECTORSTORE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e64af99b-294f-40a5-ba5d-809ae8396ede",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: torch.Size([1, 768])\n",
      "Embedding: tensor([[-1.5627e-02, -3.0900e-01, -8.8487e-01, -1.8523e-01,  8.5564e-01,\n",
      "          2.2566e-03, -3.6019e-02,  1.0778e+00, -4.0582e-01, -1.0489e-01,\n",
      "         -5.3202e-01,  8.3078e-01, -1.2765e+00, -1.0724e-01, -5.3104e-01,\n",
      "          3.6555e-01,  5.5917e-01, -6.9107e-01,  4.8836e-01, -2.3172e-01,\n",
      "         -7.5056e-01, -3.9663e-01,  1.5038e-01,  4.8764e-01,  7.5376e-02,\n",
      "         -4.3533e-01, -2.8811e-01,  3.9659e-01, -8.2029e-01, -7.9105e-01,\n",
      "          5.1959e-02, -1.2434e-01,  3.4547e-02, -8.4559e-01, -8.3917e-01,\n",
      "          4.2003e-01, -9.8905e-01,  4.4341e-02, -7.1568e-01,  3.9877e-01,\n",
      "         -3.4694e-01, -2.2894e-01, -3.5760e-01,  9.6516e-01, -9.9262e-01,\n",
      "         -1.4718e-01, -9.1589e-01,  4.8273e-01,  8.5412e-03, -2.8197e-01,\n",
      "         -6.3935e-01,  4.2913e-01,  5.7181e-01, -3.0930e-01,  1.2736e-01,\n",
      "          4.4413e-01, -8.0833e-02, -8.9270e-01, -8.1229e-01, -1.4990e-02,\n",
      "          7.2220e-01,  2.6272e-01,  1.1360e-01, -2.4762e-01,  4.1625e-01,\n",
      "          3.4310e-01,  1.1247e-01,  1.4853e-01, -8.8340e-01, -1.7478e-01,\n",
      "         -4.4056e-01, -1.2488e-01, -1.0868e+00,  8.4159e-02, -6.7464e-01,\n",
      "         -6.7289e-01,  4.5601e-01,  8.7476e-01,  4.9488e-01,  7.2610e-01,\n",
      "         -4.9290e-01, -2.2398e-01,  4.8663e-01,  6.2731e-01,  3.5819e-01,\n",
      "         -7.8301e-01, -6.0053e-01,  1.3085e-01, -5.0180e-01, -7.0902e-02,\n",
      "         -8.2867e-01, -8.9538e-01,  4.4684e-01,  5.1955e-01,  7.9017e-01,\n",
      "         -3.0336e-01,  5.8975e-01, -7.3558e-01,  6.9177e-02,  2.0903e-01,\n",
      "         -8.0115e-01, -3.1671e-01, -8.1405e-02,  1.9733e-01, -7.7513e-01,\n",
      "          1.7789e-01,  3.0971e-01,  9.9228e-02, -4.8295e-01,  1.2541e-01,\n",
      "         -8.8254e-01, -1.8115e-01, -4.4246e-01, -4.9902e-01, -8.1402e-01,\n",
      "          2.9735e-01,  8.5109e-01,  2.5892e-01,  1.7433e-01, -2.4629e-01,\n",
      "          6.6022e-01,  6.9400e-01,  5.1702e-01,  1.1670e-01,  6.6298e-01,\n",
      "          2.1431e-01,  4.7514e-01,  5.6599e-02, -4.2038e-01, -5.4389e-01,\n",
      "          4.5280e-01, -1.1108e-01,  3.0047e-01, -8.1900e-01, -3.9003e-01,\n",
      "         -3.3249e-01, -3.5469e-01, -2.2086e-01,  4.4647e-01, -1.7595e-02,\n",
      "          1.2566e-01, -2.7227e-01,  5.4058e-01, -8.0143e-01,  3.7765e-01,\n",
      "         -5.0598e-01, -5.7388e-01, -5.2683e-01, -5.0424e-01,  8.9480e-02,\n",
      "          4.2267e-02,  3.6170e-01,  5.4675e-01, -8.7912e-01, -3.4183e-01,\n",
      "          5.0605e-01,  5.7616e-01, -5.1685e-01,  2.3567e-02,  4.5782e-01,\n",
      "          5.9295e-01,  1.2826e-01,  6.8160e-02,  1.1566e-01, -1.7304e-01,\n",
      "         -7.3840e-01,  4.0109e-01, -9.1746e-02,  3.1137e-01,  1.7928e-01,\n",
      "         -5.4638e-01, -5.8655e-01, -4.1497e-03, -7.2874e-01, -1.8514e-01,\n",
      "          6.5163e-01,  6.7701e-01,  2.4599e-01, -2.6214e-01,  2.1204e-01,\n",
      "         -5.1163e-01,  7.8325e-01, -2.6554e-02,  4.3226e-01, -6.6689e-01,\n",
      "         -3.6005e-01,  5.3648e-01, -8.3864e-01, -1.5122e-01,  5.9652e-01,\n",
      "         -1.7256e-01, -6.0234e-01,  5.5983e-01, -6.2385e-01,  8.2545e-01,\n",
      "         -6.8559e-01, -5.3291e-01,  7.4534e-01,  4.5459e-01,  3.3926e-02,\n",
      "          5.9754e-01, -2.8633e-02,  4.3484e-01, -9.0126e-01, -5.0661e-01,\n",
      "         -2.6546e-01,  3.1114e-01, -6.4598e-01,  3.6085e-01,  4.7979e-01,\n",
      "         -4.1313e-01, -8.6634e-01,  7.9616e-02, -1.0626e-02,  3.5262e-01,\n",
      "         -2.3034e-01,  1.8264e-01, -1.6977e-01,  4.8274e-01, -2.4659e-01,\n",
      "          5.4885e-01, -4.9878e-01, -5.4462e-02,  3.7917e-01,  1.5701e-01,\n",
      "          5.0399e-01,  8.4820e-01, -3.8296e-01, -2.5158e-02, -2.3145e-01,\n",
      "         -5.3753e-01,  1.5953e-01, -3.1685e-01, -4.0951e-01, -6.2473e-01,\n",
      "         -1.7775e-02,  1.0294e-01, -8.6832e-02,  8.7319e-01, -5.0996e-01,\n",
      "         -4.5788e-01,  5.4779e-01, -4.3975e-01,  6.4519e-01, -1.8888e-01,\n",
      "         -2.1220e-02,  5.0784e-01, -7.2915e-01, -3.3002e-01, -2.1530e-01,\n",
      "         -7.2347e-01, -7.5185e-01,  1.7612e-01, -2.9861e-01,  1.6852e-01,\n",
      "         -9.6188e-01,  6.4780e-02,  7.8780e-01,  1.2530e+00,  4.3950e-01,\n",
      "          1.1022e-01,  6.5493e-01, -5.9792e-01, -1.4545e-01,  3.8471e-02,\n",
      "         -9.0474e-01, -1.7327e-01, -5.6518e-01,  6.4215e-01,  7.3461e-01,\n",
      "          1.0005e+00,  3.6529e-01,  1.0108e+00,  3.6435e-01,  5.1269e-01,\n",
      "         -7.0814e-01,  8.5732e-01,  7.6238e-01, -6.5521e-01, -5.7069e-02,\n",
      "         -3.8464e-01,  1.3978e+00, -5.7920e-02, -2.8012e-01,  4.4680e-01,\n",
      "         -5.6598e-01,  4.1993e-01, -5.4558e-01, -5.2034e-01,  2.7313e-01,\n",
      "         -2.9223e-01,  4.9258e-01, -8.5580e-01, -2.0585e-01,  8.4265e-01,\n",
      "         -4.5887e-01,  1.4545e-02,  1.0143e-01, -3.4624e-01, -3.7143e-03,\n",
      "          2.6051e-01,  8.6101e-01,  4.9450e-01,  4.3950e-01,  7.1798e-01,\n",
      "         -3.8382e-01,  1.4183e-01, -2.4973e-01, -2.3788e-01, -3.1594e-02,\n",
      "          1.3733e-01, -3.4003e-01,  3.5560e-01,  6.4403e-01,  5.0940e-01,\n",
      "          4.0682e-01, -6.8014e-01, -7.4868e-01, -4.4246e-01,  2.9578e-02,\n",
      "         -5.0888e-02,  7.9235e-01,  1.2991e-01, -4.8448e-01, -3.3570e-01,\n",
      "         -2.7013e-01, -5.9400e-01,  4.9412e-01, -3.3352e-01, -8.0254e-02,\n",
      "         -6.8900e-01,  9.3500e-01, -3.5968e-01,  5.7241e-01, -6.5319e-01,\n",
      "         -5.6946e-01, -2.9518e-01, -1.4558e-01, -7.2762e-01,  3.4078e-02,\n",
      "          3.3570e-02,  3.6710e-01,  4.4992e-01, -5.4546e-01,  9.5631e-01,\n",
      "         -4.4542e-01, -9.4567e-02, -5.6877e-01, -5.4296e-01,  2.5007e-02,\n",
      "         -1.3039e-01,  5.1487e-01,  3.3511e-01, -6.4031e-01, -3.2616e-01,\n",
      "         -1.2777e-01, -2.1794e-01,  7.1639e-02,  2.9147e-03,  8.1993e-02,\n",
      "          1.5169e-01,  1.3281e-01, -9.3915e-01, -3.6409e-02,  1.5859e-01,\n",
      "          1.7848e-01,  1.3364e-01, -2.1446e-01,  6.2436e-01, -2.9735e-01,\n",
      "         -1.0512e-01, -8.1355e-01, -4.6747e-01, -5.5969e-01, -4.5641e-01,\n",
      "         -8.8452e-01,  5.8181e-01,  3.4418e-01, -5.7801e-01, -2.2711e-01,\n",
      "         -4.6454e-01, -1.9020e+00, -4.5456e-01,  8.3421e-01, -1.3764e-01,\n",
      "          1.3242e-01,  1.6001e-01,  9.2909e-01, -9.7187e-01,  2.0532e-01,\n",
      "          1.4904e-01,  3.9600e-01, -6.4364e-01, -6.8532e-01, -3.9074e-01,\n",
      "          2.1314e-02, -6.6804e-01, -4.9405e-01,  8.3888e-01, -1.1012e+00,\n",
      "         -6.1591e-01,  6.7816e-01,  1.2814e+00,  4.8497e-01,  1.8304e-01,\n",
      "          2.9981e-01,  7.7969e-01, -3.5745e-01,  7.1876e-01, -1.0063e+00,\n",
      "         -3.0554e-01, -3.8931e-01,  4.0862e-01, -1.4351e-01, -2.1092e-02,\n",
      "          4.6942e-01,  8.6739e-01,  2.2852e-01,  4.8145e-01, -8.3808e-02,\n",
      "          4.9155e-01, -6.3511e-01,  4.2169e-01, -2.9608e-01,  3.5014e-01,\n",
      "          4.3400e-01, -1.5186e-01,  2.4133e-01, -8.5385e-01,  5.2648e-01,\n",
      "         -2.2849e-01, -4.7281e-01, -4.2573e-01,  3.1217e-02, -1.4410e-01,\n",
      "         -4.3963e-01,  2.8349e-01, -1.2872e-01, -5.5076e-01, -4.4229e-01,\n",
      "          3.6755e-01,  2.5736e-01,  2.5192e-01,  3.6495e-01,  5.2002e-01,\n",
      "         -4.3845e-01, -4.3048e-01, -4.9365e-01,  5.6645e-01,  2.0888e-01,\n",
      "          3.4086e-01, -2.7030e-01,  3.8324e-01,  3.3051e-01, -4.7510e-01,\n",
      "          5.2811e-01, -2.0765e-02,  6.1812e-02,  2.4998e-01, -2.2400e-01,\n",
      "          2.6919e-01, -1.7154e-01, -2.6268e-01, -6.7581e-01,  1.0902e+00,\n",
      "         -1.9984e-01,  3.4448e-02, -3.4805e-01,  2.9491e-01, -5.5409e-01,\n",
      "         -7.9426e-01,  5.2583e-02,  4.9559e-01,  1.0554e+00,  1.6309e-01,\n",
      "         -5.2301e-01, -3.1921e-01,  2.6565e-01,  3.3244e-01, -4.3768e-02,\n",
      "         -4.8653e-01,  3.0717e-01, -5.8862e-01, -2.6926e-01, -3.7730e-01,\n",
      "         -3.0093e-01, -1.5762e-01,  3.8224e-01, -7.9950e-02, -8.3924e-02,\n",
      "         -8.5207e-01,  6.8912e-01,  4.3869e-01, -2.8727e-01,  1.1361e-01,\n",
      "         -2.0571e-01, -4.2009e-01,  8.0901e-01,  2.1433e-01,  6.0443e-01,\n",
      "          4.9038e-01,  4.8745e-02, -5.1103e-01, -1.9354e-01,  1.1460e-02,\n",
      "          2.3315e-01, -3.6168e-01,  2.5276e-01, -1.7494e-01, -3.1308e-01,\n",
      "         -9.4552e-01,  4.9779e-01,  3.0602e-01, -6.2693e-01,  8.0418e-02,\n",
      "         -2.7623e-01, -3.3024e-01,  2.4290e-01, -1.6830e-01, -5.0925e-01,\n",
      "         -4.2870e-01,  5.2711e-01,  1.1831e-02, -2.7162e-01, -1.0424e-01,\n",
      "          3.9207e-01,  2.9233e-01, -6.5543e-01,  2.0021e-01, -3.6145e-01,\n",
      "         -3.2607e-01, -7.4764e-01, -6.1265e-01,  6.3325e-02,  1.9373e-01,\n",
      "         -4.6513e-01, -2.9809e-01, -5.2031e-01,  1.0194e+00,  4.9058e-01,\n",
      "         -6.1942e-01,  8.6225e-01, -4.3076e-02, -1.7070e-01,  2.2290e-01,\n",
      "          9.3738e-01, -5.2850e-02,  5.1614e-01,  1.9832e-01,  4.1001e-01,\n",
      "         -4.4436e-01, -4.9806e-01, -2.5781e-01,  5.5156e-01,  4.8579e-01,\n",
      "         -4.7678e-01, -4.2153e-01,  2.9348e-01, -1.2044e+00, -2.3823e-01,\n",
      "         -3.5478e-01,  6.2632e-02,  4.8781e-01, -2.8820e-01, -1.9064e-01,\n",
      "          7.6711e-01, -6.2937e-01, -7.6896e-01, -3.7701e-01,  4.3545e-01,\n",
      "         -8.9498e-01,  4.1918e-01,  4.6471e-01,  2.7189e-01, -5.8680e-01,\n",
      "         -5.1785e-01,  7.8261e-01,  6.8838e-01,  4.8779e-01, -3.8500e-01,\n",
      "          8.4999e-01,  7.8418e-01,  6.9038e-01,  4.1430e-01, -6.8344e-01,\n",
      "          4.2096e-01,  6.8078e-01, -5.7285e-01, -2.1276e-01,  8.9379e-01,\n",
      "         -3.1994e-01,  4.3409e-01,  6.7379e-01, -1.0046e+00,  7.9178e-01,\n",
      "          3.4087e-01, -4.9963e-01, -9.5252e-03, -1.1668e-01,  6.5506e-01,\n",
      "          5.0119e-01, -2.5556e-01,  1.8459e-01,  1.9367e-02,  3.0361e-01,\n",
      "          5.5079e-01, -8.5130e-02,  4.5961e-01, -3.6671e-01,  8.2320e-01,\n",
      "          1.5313e-01, -1.7267e-01, -6.0115e-01,  4.5260e-01, -4.6301e-01,\n",
      "         -2.0377e-01,  7.3010e-01, -3.8095e-01, -9.5880e-02, -4.9337e-01,\n",
      "          3.8963e-01,  5.4790e-01, -4.1281e-01,  4.8756e-01,  3.0922e-01,\n",
      "         -8.6564e-01, -2.3508e-01,  5.5648e-01, -2.6039e-01, -3.8836e-01,\n",
      "          6.9359e-01,  9.2593e-01,  1.2822e-01,  4.7291e-03, -1.2257e-01,\n",
      "          4.7391e-01, -5.7238e-01,  9.8717e-01,  4.0653e-01,  9.1721e-01,\n",
      "          1.8110e-01,  7.5641e-01, -5.1958e-01, -4.7846e-01,  6.4022e-01,\n",
      "         -1.6430e-01,  3.0483e-01,  9.5943e-01,  3.3719e-01,  6.1277e-01,\n",
      "          4.3733e-01,  8.9549e-01,  1.4178e-01,  4.5612e-01,  9.8192e-01,\n",
      "         -1.2781e-01,  6.7885e-01,  3.5626e-01,  9.4704e-01, -3.6470e-01,\n",
      "          5.2235e-02, -7.1333e-01,  3.8232e-01,  1.8889e-01,  4.6575e-01,\n",
      "          2.4740e-01, -1.9681e-01,  2.3362e-01, -8.0158e-01, -2.7419e-01,\n",
      "          1.2026e+00,  2.3224e-01, -2.5691e-01, -1.4656e-01,  6.5247e-01,\n",
      "         -8.3005e-01, -4.5106e-02,  5.2711e-01, -1.4585e-01,  7.8625e-01,\n",
      "          1.8151e-01,  6.9135e-01,  3.9620e-01,  2.0250e-01,  5.3636e-01,\n",
      "          9.3342e-02, -4.6557e-01,  9.7117e-02,  7.0473e-01, -3.2711e-01,\n",
      "         -1.2716e-01, -1.0310e+00,  8.9809e-04, -3.1751e-01,  1.8051e-03,\n",
      "          2.8308e-01, -6.9240e-01, -2.8921e-01, -6.8395e-01,  8.4788e-02,\n",
      "          4.9223e-01, -3.8921e-01, -1.8952e-01, -5.2930e-01,  2.2879e-01,\n",
      "         -4.8222e-02, -2.9705e-01,  4.6557e-01,  3.3060e-01,  1.3019e-01,\n",
      "         -2.0012e-01, -7.1658e-01, -4.9575e-01,  3.6518e-01, -4.2949e-01,\n",
      "          7.3794e-01, -4.8211e-01,  3.8084e-01,  4.7667e-01, -5.6262e-01,\n",
      "         -5.3215e-01,  2.1074e-01, -2.8422e-01,  3.1892e-01,  8.2985e-01,\n",
      "          4.7637e-01,  6.1882e-01, -1.9700e-01, -3.7756e-01, -3.7324e-01,\n",
      "         -5.4063e-01,  9.2452e-01, -2.9397e-01,  1.0021e+00,  5.7988e-01,\n",
      "          4.7317e-01, -4.8580e-01,  2.3823e-02, -9.8180e-01,  1.1220e-01,\n",
      "         -7.7156e-01,  4.6635e-01,  1.3443e+00, -8.2640e-01,  4.8914e-02,\n",
      "          7.9745e-01,  1.8162e-01, -3.9476e-01,  3.1934e-01,  5.8582e-01,\n",
      "          2.2283e-01,  3.0062e-01,  3.5432e-01,  1.4284e-01, -4.7315e-01,\n",
      "         -8.0542e-01, -6.7224e-01,  7.7826e-01,  3.5610e-01, -4.7265e-02,\n",
      "          3.8078e-01, -4.2923e-01,  4.1757e-01, -1.5861e-02, -2.5219e-01,\n",
      "         -6.4783e-03,  2.2433e-01,  9.0020e-01]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"intfloat/e5-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Define a test sentence (prefix with \"query:\" or \"passage:\" as required)\n",
    "text = \"query: What is artificial intelligence?\"\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state[:, 0]  # CLS token embedding\n",
    "\n",
    "print(\"Embedding shape:\", embeddings.shape)\n",
    "print(\"Embedding:\", embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fddc4e2e-6229-4d0b-8487-275bee5a4e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# This downloads the model and all required files\n",
    "model = SentenceTransformer(\"intfloat/e5-base-v2\")\n",
    "\n",
    "# Save it locally\n",
    "model.save(\"./e5-base-v2-local\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b09bb19-1f79-4d7d-a829-338b53d9e1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'zip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!zip -r e5-base-v2-local.zip e5-base-v2-local\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8c3c0a1-b1e2-4e9f-ac1a-4eeb1b7d3d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./e5-base-v2-local\\\\tokenizer_config.json',\n",
       " './e5-base-v2-local\\\\special_tokens_map.json',\n",
       " './e5-base-v2-local\\\\vocab.txt',\n",
       " './e5-base-v2-local\\\\added_tokens.json',\n",
       " './e5-base-v2-local\\\\tokenizer.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model = AutoModel.from_pretrained(\"intfloat/e5-base-v2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"intfloat/e5-base-v2\")\n",
    "\n",
    "model.save_pretrained(\"./e5-base-v2-local\")\n",
    "tokenizer.save_pretrained(\"./e5-base-v2-local\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694fdb9a-b091-447f-b148-737cdf60ec0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
